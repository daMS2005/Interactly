{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:\n",
    "https://git.gesis.org/papenmaa/chiir21_naturallanguagequeries/-/tree/master/VACOS_NLQ_v2?ref_type=heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3481 entries, 0 to 3480\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   ID           3481 non-null   int64 \n",
      " 1   domain       3481 non-null   object\n",
      " 2   text         3481 non-null   object\n",
      " 3   user         3481 non-null   object\n",
      " 4   key facts    3481 non-null   object\n",
      " 5   vague words  3481 non-null   object\n",
      " 6   text token   3481 non-null   object\n",
      " 7   negations    3481 non-null   object\n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 217.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>domain</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "      <th>key facts</th>\n",
       "      <th>vague words</th>\n",
       "      <th>text token</th>\n",
       "      <th>negations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>718</td>\n",
       "      <td>laptop</td>\n",
       "      <td>I want a smaller laptop, nothing too unwieldy....</td>\n",
       "      <td>{'age': 34, 'gender': 'male', 'domain knowledg...</td>\n",
       "      <td>{'IAA key facts': 'n/a', 'annotations': [{'wor...</td>\n",
       "      <td>{'IAA vague words': 'n/a', 'annotations': [{'w...</td>\n",
       "      <td>[[0, I], [2, want], [7, a], [9, smaller], [17,...</td>\n",
       "      <td>[{'word': 'nothing', 'start index': 25, 'keyfa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>719</td>\n",
       "      <td>laptop</td>\n",
       "      <td>I would like it to look sleek, and probably co...</td>\n",
       "      <td>{'age': 40, 'gender': 'female', 'domain knowle...</td>\n",
       "      <td>{'IAA key facts': 'n/a', 'annotations': [{'wor...</td>\n",
       "      <td>{'IAA vague words': 'n/a', 'annotations': [{'w...</td>\n",
       "      <td>[[0, I], [2, would], [8, like], [13, it], [16,...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>720</td>\n",
       "      <td>laptop</td>\n",
       "      <td>I think l would look at the same make of .lapt...</td>\n",
       "      <td>{'age': 64, 'gender': 'female', 'domain knowle...</td>\n",
       "      <td>{'IAA key facts': 'n/a', 'annotations': [{'wor...</td>\n",
       "      <td>{'IAA vague words': 'n/a', 'annotations': [{'w...</td>\n",
       "      <td>[[0, I], [2, think], [8, l], [10, would], [16,...</td>\n",
       "      <td>[{'word': 'not', 'start index': 372, 'keyfact ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>721</td>\n",
       "      <td>laptop</td>\n",
       "      <td>i would buy a resonable priced laptop with all...</td>\n",
       "      <td>{'age': 43, 'gender': 'female', 'domain knowle...</td>\n",
       "      <td>{'IAA key facts': 'n/a', 'annotations': [{'wor...</td>\n",
       "      <td>{'IAA vague words': 'n/a', 'annotations': [{'w...</td>\n",
       "      <td>[[0, i], [2, would], [8, buy], [12, a], [14, r...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>723</td>\n",
       "      <td>laptop</td>\n",
       "      <td>It would be a faster version with more memory ...</td>\n",
       "      <td>{'age': 41, 'gender': 'female', 'domain knowle...</td>\n",
       "      <td>{'IAA key facts': 'n/a', 'annotations': [{'wor...</td>\n",
       "      <td>{'IAA vague words': 'n/a', 'annotations': [{'w...</td>\n",
       "      <td>[[0, It], [3, would], [9, be], [12, a], [14, f...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID  domain                                               text  \\\n",
       "0  718  laptop  I want a smaller laptop, nothing too unwieldy....   \n",
       "1  719  laptop  I would like it to look sleek, and probably co...   \n",
       "2  720  laptop  I think l would look at the same make of .lapt...   \n",
       "3  721  laptop  i would buy a resonable priced laptop with all...   \n",
       "4  723  laptop  It would be a faster version with more memory ...   \n",
       "\n",
       "                                                user  \\\n",
       "0  {'age': 34, 'gender': 'male', 'domain knowledg...   \n",
       "1  {'age': 40, 'gender': 'female', 'domain knowle...   \n",
       "2  {'age': 64, 'gender': 'female', 'domain knowle...   \n",
       "3  {'age': 43, 'gender': 'female', 'domain knowle...   \n",
       "4  {'age': 41, 'gender': 'female', 'domain knowle...   \n",
       "\n",
       "                                           key facts  \\\n",
       "0  {'IAA key facts': 'n/a', 'annotations': [{'wor...   \n",
       "1  {'IAA key facts': 'n/a', 'annotations': [{'wor...   \n",
       "2  {'IAA key facts': 'n/a', 'annotations': [{'wor...   \n",
       "3  {'IAA key facts': 'n/a', 'annotations': [{'wor...   \n",
       "4  {'IAA key facts': 'n/a', 'annotations': [{'wor...   \n",
       "\n",
       "                                         vague words  \\\n",
       "0  {'IAA vague words': 'n/a', 'annotations': [{'w...   \n",
       "1  {'IAA vague words': 'n/a', 'annotations': [{'w...   \n",
       "2  {'IAA vague words': 'n/a', 'annotations': [{'w...   \n",
       "3  {'IAA vague words': 'n/a', 'annotations': [{'w...   \n",
       "4  {'IAA vague words': 'n/a', 'annotations': [{'w...   \n",
       "\n",
       "                                          text token  \\\n",
       "0  [[0, I], [2, want], [7, a], [9, smaller], [17,...   \n",
       "1  [[0, I], [2, would], [8, like], [13, it], [16,...   \n",
       "2  [[0, I], [2, think], [8, l], [10, would], [16,...   \n",
       "3  [[0, i], [2, would], [8, buy], [12, a], [14, r...   \n",
       "4  [[0, It], [3, would], [9, be], [12, a], [14, f...   \n",
       "\n",
       "                                           negations  \n",
       "0  [{'word': 'nothing', 'start index': 25, 'keyfa...  \n",
       "1                                                 []  \n",
       "2  [{'word': 'not', 'start index': 372, 'keyfact ...  \n",
       "3                                                 []  \n",
       "4                                                 []  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('Input_Data/Product_NER_Dataset.json')\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows without 'annotations' in their 'key facts' column\n",
    "df = df[df['key facts'].notna()]\n",
    "\n",
    "def has_annotations(x):\n",
    "    if 'annotations' in x.keys():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "df = df[df['key facts'].apply(lambda x: has_annotations(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can create a new column for the annotations of each row\n",
    "def get_annotations(x):\n",
    "    return x['annotations']\n",
    "df['annotations'] = df['key facts'].apply(lambda x: get_annotations(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each item in the annotations list is a dictionary with metadata about the named entities\n",
    "# For the sake of our training, we only want entities with the attribute category of 'meta', 'meta_brand', or 'meta_model'\n",
    "def has_meta(x):\n",
    "    if x['attribute category'] in ['meta', 'meta_brand', 'meta_model']:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Since each annotation is a list of dicts for each entity, we need to filter those lists to remove words that don't have the meta categories\n",
    "def filter_annotations(x):\n",
    "    return [y for y in x if has_meta(y)]\n",
    "df['annotations'] = df['annotations'].apply(lambda x: filter_annotations(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     [{'word': 'mac', 'start index': 233, 'attribut...\n",
      "6     [{'word': 'mac', 'start index': 26, 'attribute...\n",
      "7     [{'word': 'reliable', 'start index': 2, 'attri...\n",
      "9     [{'word': 'HP', 'start index': 94, 'attribute ...\n",
      "10    [{'word': 'Apple', 'start index': 16, 'attribu...\n",
      "Name: annotations, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Each annotation dictionary also has information we won't need\n",
    "# We can extract the word, start index, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will only need the text and text annotations for our training\n",
    "df = df[['text', 'annotations']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'word': 'mac', 'start index': 233, 'attribute category': 'meta_model', 'keyfact index': 4, 'attribute or value': 'value', 'BIO scheme': 'B', 'vogue': False}, {'word': 'macbook', 'start index': 267, 'attribute category': 'meta_model', 'keyfact index': 5, 'attribute or value': 'value', 'BIO scheme': 'B', 'vogue': False}]\n",
      "I would like it to look sleek, and probably cost more than the one that just broke.  I would like it to have a good screen and screen.  I would like a good brand.  I used microsoft computers all my life and just recently switched to mac.  I think I would buy another macbook.\n"
     ]
    }
   ],
   "source": [
    "df.head()\n",
    "attr = df.iloc[0]['annotations']\n",
    "print(attr)\n",
    "print(df.iloc[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Within the annotations, we only need the word, start index and BIO scheme\n",
    "def filter_annotations(annotations):\n",
    "    return [{'word': ann['word'], 'start index': ann['start index'], 'BIO scheme': ann['BIO scheme']} for ann in annotations]\n",
    "\n",
    "df['annotations'] = df['annotations'].apply(filter_annotations)\n",
    "\n",
    "df.to_csv('Cleaned_Data/Product_NER_Dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([749, 128]) torch.Size([749, 128]) torch.Size([749, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "\n",
    "def tokenize_and_align_labels(df):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        annotations = row['annotations']  # This is the list of annotations with 'word', 'start index', and 'BIO scheme'\n",
    "\n",
    "        # Tokenize the input text with the fast tokenizer\n",
    "        encoding = tokenizer(text, truncation=True, padding='max_length', max_length=128, return_offsets_mapping=True)\n",
    "\n",
    "        # Create a list for BIO labels (initialize with 'O')\n",
    "        bio_labels = ['O'] * len(encoding['input_ids'])\n",
    "\n",
    "        # Align the BIO labels\n",
    "        for annotation in annotations:\n",
    "            word = annotation['word']\n",
    "            bio_scheme = annotation['BIO scheme']\n",
    "            start_index = annotation['start index']\n",
    "\n",
    "            # Find the tokens corresponding to the start of the word\n",
    "            for i, (token, offset) in enumerate(zip(encoding['input_ids'], encoding['offset_mapping'])):\n",
    "                token_str = tokenizer.decode([token]).strip()\n",
    "\n",
    "                # Check if the token matches the word and is within the correct range\n",
    "                if text[offset[0]:offset[1]].lower() == word.lower():\n",
    "                    if bio_scheme == 'B':\n",
    "                        bio_labels[i] = 'B'\n",
    "                    elif bio_scheme == 'I':\n",
    "                        bio_labels[i] = 'I'\n",
    "                    break\n",
    "\n",
    "                # If it's a continuation of the word, propagate the 'I' label\n",
    "                if bio_scheme == 'B' and token_str.startswith('##'):\n",
    "                    bio_labels[i] = 'I'\n",
    "\n",
    "        # Convert BIO labels to numeric values\n",
    "        bio_label_to_id = {'O': 0, 'B': 1, 'I': 2}\n",
    "        aligned_labels = [bio_label_to_id[label] for label in bio_labels]\n",
    "\n",
    "        # Add the results to the lists\n",
    "        input_ids.append(encoding['input_ids'])\n",
    "        attention_masks.append(encoding['attention_mask'])\n",
    "        labels.append(aligned_labels)\n",
    "\n",
    "    # Convert lists to tensors for PyTorch\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "input_ids, attention_masks, labels = tokenize_and_align_labels(df)\n",
    "\n",
    "# Check the shapes of the tensors\n",
    "print(input_ids.shape, attention_masks.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tensors to the NER dataset\n",
    "torch.save(input_ids, 'Cleaned_Data/NER/input_ids.pt')\n",
    "torch.save(attention_masks, 'Cleaned_Data/NER/attention_masks.pt')\n",
    "torch.save(labels, 'Cleaned_Data/NER/labels.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
